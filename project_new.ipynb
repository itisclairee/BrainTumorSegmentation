{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from kmeans import KMeans\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "import matplotlib.patches as mpatches\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from scipy.ndimage import rotate\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from IPython.display import display\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BraTS (Brain Tumour Segmentation) dataset consists of multi-modal MRI images of brain tumours, specifically gliomas, a subtype which represents about 80% of all malignant brain tumours, and the corresponding segmentation masks. The dataset is updated yearly in the context of the MICCAI (Medical Image Computing and Computer-Assisted Intervention) conference. The latest data available is from BraTS2023, but we chose to work with BraTS2020. This was done because the size of the 2020 dataset, at 369 patients, is already enough to make the computations needed very lenghty on the systems at our disposal. BraTS2021 and BraTS2022 comprise data from more than 2000 patients while BraTS2023 goes up to 4500 patients, which were computationally unfeasible with the computers at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BraTS2020 dataset is made up of routine clinically-acquired pre-operative multimodal MRI of glioblastoma and lower-grade glioma with accompanying ground truth labels by board-certified neuroradiologists. The scans have been taken at 19 different insitutions and come in the NIfTI format (.nii.gz), a standard format for MRI images. They are multi-modal in the sense that, for each of the 369 patients, 4 different MRI configurations have been used:\n",
    "\n",
    "a) native (T1) and b) post-contrast T1-weighted (T1Gd)\n",
    "\n",
    "c) T2-weighted (T2), and d) T2 Fluid Attenuated Inversion Recovery (T2-FLAIR)\n",
    "\n",
    "T1 and T2 refer to longitudinal and transversal relaxing time respectively and are the two main focus of MRI imaging. Through the differences in the longitudinal and transversal relaxing time one can distinguish a certain type of biological tissue from another. In T1 scans, white matter appears brilliant, grey matter appears grey and liquids and lesions appear darker. T1Gd is when a T1 scan is used together with a Gadolinium-based contrast which, being absorbed by those tissues with a compromised blood-brain barrier, allows to easily distinguish the active part of the tumour (called enhancing tumour). In T2 scans, on the countrary, liquid and lesions are brilliant and white matter is dark. T2-FLAIR is a technique which allows to darken the cerebrospinal fluid (CSF) making it easier to distinguish the edema due to the tumour.\n",
    "\n",
    "Data from a mix of these modalities allows for a correct segmentation of the tumour tissue and is fundamental for the practical work of neurosurgeons and neuroradiologists, which need to understand the geometry and composition of the tumour. This kind of segmentation task is made more difficult by the heterogeneity of gliomas and the different tissues making up a tumour. Three different patological tissues are considered here: A) Enhancing Tumour Core, which represents the active part of the disease and has label 4, B) Necrotic and Non-Enhancing Tumour Core, which are inactive parts of the tumour and are labelled 1, and C) Peritumoral Edema, which is fluid that accumulates around the tumour and is labelled with 2. The fourth and last label, 0, is used for everything else (the rest of the brain with all its structures and CSF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Scans with Nibabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with NIfTI (Neuroimaging Informatics Technology Initiative) data one can use the Nibabel library, which has been created to easily access and manipulate neurological data in various medical formats. We use it to extract the scans voxel values into a NumPy array, allowing us to work with more familiar objects afterwards.\n",
    "\n",
    "Each set of scans has 155 slices with 240x240 pixels per slice (so we have 240x240x155 voxels per modality). We have 4 modalities per patient and 369 patients in total. In the cell below we demonstrate how to load the modalities from patient 355. We also rescale the images as this format can have voxel intensity values which vary from 0 to a few thousands and we want to keep all modalities in the same scale so that the model can confront slices without any bias due only to the differing scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory di Chiara\n",
    "#TRAIN_DATASET_PATH = \"/Users/chiaracangelosi/Documents/1-Uni/DataScience/Dataset/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\"\n",
    "\n",
    "#Directory di Gianluca\n",
    "TRAIN_DATASET_PATH= \"C:/Users/g2not/Desktop/Università/Data Mining and Machine Learning/Progetto/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\"\n",
    "\n",
    "# Carica il file .nii come array NumPy sfruttando nibabel\n",
    "#test_image_flair_ = nib.load(TRAIN_DATASET_PATH + \"BraTS20_Training_355/BraTS20_Training_355_flair.nii\")\n",
    "#print(test_image_flair_.header)\n",
    "\n",
    "test_image_flair = nib.load(TRAIN_DATASET_PATH + \"BraTS20_Training_355/BraTS20_Training_355_flair.nii\").get_fdata()\n",
    "\n",
    "# Informazioni sull'immagine\n",
    "print(\"FLAIR Shape: \", test_image_flair.shape)\n",
    "print(\"FLAIR Dtype: \", test_image_flair.dtype)\n",
    "print(\"FLAIR Min: \", test_image_flair.min())\n",
    "print(\"FLAIR Max: \", test_image_flair.max())\n",
    "\n",
    "# Rescaling voxel-wise a [0, 1]\n",
    "test_image_flair = (test_image_flair - np.min(test_image_flair)) / (np.max(test_image_flair) - np.min(test_image_flair))\n",
    "\n",
    "# Verifica\n",
    "print(\"Rescaled min:\", test_image_flair.min())\n",
    "print(\"Rescaled max:\", test_image_flair.max(), \"\\n\")\n",
    "\n",
    "#Carichiamo e riscaliamo alla stessa maniera anche le altre modalità\n",
    "test_image_t1 = nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_355/BraTS20_Training_355_t1.nii').get_fdata()\n",
    "print(\"T1 Shape: \", test_image_t1.shape)\n",
    "print(\"T1 Min: \", test_image_t1.min())\n",
    "print(\"T1 Max: \", test_image_t1.max(), \"\\n\")\n",
    "\n",
    "test_image_t1 = (test_image_t1 - np.min(test_image_t1)) / (np.max(test_image_t1) - np.min(test_image_t1))\n",
    "\n",
    "test_image_t1ce = nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_355/BraTS20_Training_355_t1ce.nii').get_fdata()\n",
    "print(\"T1ce Shape: \", test_image_t1ce.shape)\n",
    "print(\"T1ce Min: \", test_image_t1ce.min())\n",
    "print(\"T1ce Max: \", test_image_t1ce.max(), \"\\n\")\n",
    "\n",
    "test_image_t1ce = (test_image_t1ce - np.min(test_image_t1ce)) / (np.max(test_image_t1ce) - np.min(test_image_t1ce))\n",
    "\n",
    "test_image_t2 = nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_355/BraTS20_Training_355_t2.nii').get_fdata()\n",
    "print(\"T2 Shape: \", test_image_t2.shape)\n",
    "print(\"T2 Min: \", test_image_t2.min())\n",
    "print(\"T2 Max: \", test_image_t2.max(), \"\\n\")\n",
    "\n",
    "test_image_t2 = (test_image_t2 - np.min(test_image_t2)) / (np.max(test_image_t2) - np.min(test_image_t2))\n",
    "\n",
    "# La maschera di segmentazione non va riscalata, in quanto i valori che assume sono 0, 1, 2 e 4 a seconda del tipo di tessuto\n",
    "# 0 = background, 1 = edema, 2 = non-enhancing tumour, 4 = enhancing tumour\n",
    "test_image_seg = nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_355/BraTS20_Training_355_seg.nii').get_fdata()\n",
    "print(\"Mask Shape: \", test_image_seg.shape)\n",
    "print(\"Mask Min: \", test_image_seg.min())\n",
    "print(\"Mask Max: \", test_image_seg.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell a dynamic visualization is built so that one can check how the different scans and the corresponding mask evolves slice by slice. In the mask, black represents healthy tissue (therefore it shows as the background for the tumoral tissue), green is the Necrotic and Non-Enhancing tissue, yellow is the Edema while light blue is used for the Enhancing Tumour Core. The visualization is again using the data we already loaded relative to patient 355."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Colori per la maschera ===\n",
    "labels = [0, 1, 2, 4]\n",
    "colors = ['black', 'green', 'yellow', 'lightblue']\n",
    "class_names = ['Background (0)', 'NCR & NET (1)', 'Edema (2)', 'Enhancing Tumor (4)']\n",
    "cmap_mask = ListedColormap(colors)\n",
    "norm = BoundaryNorm([0, 0.5, 1.5, 3, 5], cmap_mask.N)\n",
    "\n",
    "# === Visualizzazione dinamica ===\n",
    "def show_slice(slice_idx):\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "    # MRI modalities\n",
    "    titles = ['T1', 'T1ce', 'T2', 'FLAIR']\n",
    "    volumes = [test_image_t1, test_image_t1ce, test_image_t2, test_image_flair]\n",
    "\n",
    "    for i in range(4):\n",
    "        axs[i // 3, i % 3].imshow(volumes[i][:, :, slice_idx], cmap='gray')\n",
    "        axs[i // 3, i % 3].set_title(titles[i])\n",
    "        axs[i // 3, i % 3].axis('off')\n",
    "\n",
    "    # Mask\n",
    "    axs[1, 1].imshow(test_image_seg[:, :, slice_idx], cmap=cmap_mask, norm=norm)\n",
    "    axs[1, 1].set_title('Mask')\n",
    "    axs[1, 1].axis('off')\n",
    "\n",
    "    # Legend\n",
    "    axs[1, 2].axis('off')\n",
    "    patches = [mpatches.Patch(color=colors[i], label=class_names[i]) for i in range(len(labels))]\n",
    "    axs[1, 2].legend(handles=patches, loc='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# === Interfaccia interattiva ===\n",
    "interact(\n",
    "    show_slice,\n",
    "    slice_idx=widgets.IntSlider(\n",
    "        value=95,\n",
    "        min=0,\n",
    "        max=test_image_t1.shape[2] - 1,\n",
    "        step=1,\n",
    "        description='Slice:',\n",
    "        continuous_update=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transverse view is not the only useful visualization of MRI scans, frontal and sagittal angles are other useful options. Below is an example of the same modality, T1, shown by slicing over the other dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione delle viste trasversa, frontale e sagittale\n",
    "def show_views(slice_idx):\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "    H, W, D = test_image_t1ce.shape  # 240, 240, 155\n",
    "    \n",
    "    # Transverse View\n",
    "    z_idx = min(slice_idx, D - 1) # Assicura di non eccedere l'indice\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    img1 = test_image_t1ce[:, :, z_idx]\n",
    "    ax1.imshow(img1, cmap='gray')\n",
    "    ax1.set_title(f'T1 - Transverse (Z={z_idx})')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Frontal View \n",
    "    y_idx = min(slice_idx, W - 1)\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    img2 = rotate(test_image_t1ce[:, y_idx, :], 90, reshape=True)\n",
    "    ax2.imshow(img2, cmap='gray')\n",
    "    ax2.set_title(f'T1 - Frontal (Y={y_idx})')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    # Sagittal View\n",
    "    x_idx = min(slice_idx, H - 1)\n",
    "    ax3 = fig.add_subplot(1, 3, 3)\n",
    "    img3 = rotate(test_image_t1ce[x_idx, :, :], 90, reshape=True)\n",
    "    ax3.imshow(img3, cmap='gray')\n",
    "    ax3.set_title(f'T1 - Sagittal (X={x_idx})')\n",
    "    ax3.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Slider interattivo\n",
    "max_index = max(test_image_t1ce.shape)  # 240\n",
    "interact(\n",
    "    show_views,\n",
    "    slice_idx=widgets.IntSlider(\n",
    "        value=95,\n",
    "        min=0,\n",
    "        max=max_index - 1,\n",
    "        step=1,\n",
    "        description='Slice:',\n",
    "        continuous_update=True\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Mappa colori e normalizzazione (adatta a BraTS ad esempio)\n",
    "labels = [0, 1, 2, 4]\n",
    "colors = ['black', 'green', 'yellow', 'lightblue']\n",
    "class_names = ['Background (0)', 'NCR/NET (1)', 'Edema (2)', 'Enhancing Tumor (4)']\n",
    "cmap = ListedColormap(colors)\n",
    "norm = BoundaryNorm([0, 0.5, 1.5, 3, 5], len(colors))\n",
    "\n",
    "# === Funzione dinamica\n",
    "def show_segmented_slice(slice_idx):\n",
    "    # Isola ogni classe come maschera separata (usando NaN per trasparenza)\n",
    "    seg_0 = test_image_seg.copy()\n",
    "    seg_0[seg_0 != 0] = np.nan\n",
    "    \n",
    "    seg_1 = test_image_seg.copy()\n",
    "    seg_1[seg_1 != 1] = np.nan\n",
    "    \n",
    "    seg_2 = test_image_seg.copy()\n",
    "    seg_2[seg_2 != 2] = np.nan\n",
    "    \n",
    "    seg_4 = test_image_seg.copy()\n",
    "    seg_4[seg_4 != 4] = np.nan\n",
    "\n",
    "    # Legenda personalizzata\n",
    "    legend = [plt.Rectangle((0, 0), 1, 1, color=cmap(i), label=class_names[i]) for i in range(len(class_names))]\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 5, figsize=(20, 5))\n",
    "\n",
    "    ax[0].imshow(test_image_seg[:, :, slice_idx], cmap=cmap, norm=norm)\n",
    "    ax[0].set_title('Original Segmentation')\n",
    "    ax[0].legend(handles=legend, loc='lower left')\n",
    "    ax[0].axis('off')\n",
    "\n",
    "    ax[1].imshow(seg_0[:, :, slice_idx], cmap=cmap, norm=norm)\n",
    "    ax[1].set_title('Class 0 (Background)')\n",
    "    ax[1].axis('off')\n",
    "\n",
    "    ax[2].imshow(seg_1[:, :, slice_idx], cmap=cmap, norm=norm)\n",
    "    ax[2].set_title('Class 1 (NCR/NET)')\n",
    "    ax[2].axis('off')\n",
    "\n",
    "    ax[3].imshow(seg_2[:, :, slice_idx], cmap=cmap, norm=norm)\n",
    "    ax[3].set_title('Class 2 (Edema)')\n",
    "    ax[3].axis('off')\n",
    "\n",
    "    ax[4].imshow(seg_4[:, :, slice_idx], cmap=cmap, norm=norm)\n",
    "    ax[4].set_title('Class 4 (Enhancing Tumor)')\n",
    "    ax[4].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# === Slider interattivo\n",
    "interact(\n",
    "    show_segmented_slice,\n",
    "    slice_idx=widgets.IntSlider(\n",
    "        value=95,\n",
    "        min=0,\n",
    "        max=test_image_seg.shape[2] - 1,\n",
    "        step=1,\n",
    "        description='Slice:',\n",
    "        continuous_update=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics K-Means and NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "def compute_metrics(true_labels, pred_labels, n_classes=4):\n",
    "    \"\"\"\n",
    "    Compute Dice, Sensitivity, Precision, and Hausdorff Distance for each class.\n",
    "    \"\"\"\n",
    "    # Remap ground truth: 4 -> 3\n",
    "    true_processed = true_labels.copy()\n",
    "    true_processed[true_processed == 4] = 3\n",
    "    \n",
    "    # Flatten arrays\n",
    "    true_flat = true_processed.flatten()\n",
    "    pred_flat = pred_labels.flatten()\n",
    "    \n",
    "    # Use Hungarian algorithm to map clusters to classes\n",
    "    conf_matrix = confusion_matrix(true_flat, pred_flat, labels=range(n_classes))\n",
    "    row_ind, col_ind = linear_sum_assignment(-conf_matrix)\n",
    "    mapping = {col: row for row, col in zip(row_ind, col_ind)}\n",
    "    mapped_pred = np.vectorize(mapping.get)(pred_flat).reshape(pred_labels.shape)\n",
    "    \n",
    "    metrics_dict = {}\n",
    "    for class_id in range(n_classes):\n",
    "        true_binary = (true_processed == class_id)\n",
    "        pred_binary = (mapped_pred == class_id)\n",
    "        \n",
    "        # Dice Coefficient\n",
    "        intersection = np.logical_and(true_binary, pred_binary).sum()\n",
    "        dice = (2. * intersection) / (true_binary.sum() + pred_binary.sum() + 1e-6)\n",
    "        \n",
    "        # Sensitivity (Recall)\n",
    "        sensitivity = intersection / (true_binary.sum() + 1e-6)\n",
    "        \n",
    "        # Precision\n",
    "        precision = intersection / (pred_binary.sum() + 1e-6)\n",
    "        \n",
    "        # Hausdorff Distance\n",
    "        hd = np.nan\n",
    "        if np.any(true_binary) and np.any(pred_binary):\n",
    "            coords_true = np.argwhere(true_binary)\n",
    "            coords_pred = np.argwhere(pred_binary)\n",
    "            hd = max(directed_hausdorff(coords_true, coords_pred)[0],\n",
    "                     directed_hausdorff(coords_pred, coords_true)[0])\n",
    "        \n",
    "        metrics_dict[class_id] = {\n",
    "            'Dice': dice,\n",
    "            'Sensitivity': sensitivity,\n",
    "            'Precision': precision,\n",
    "            'Hausdorff Distance': hd\n",
    "        }\n",
    "    \n",
    "    return metrics_dict, mapped_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_segmentation_comparison(true_slice, pred_slice, metrics_dict, method_name, slice_idx):\n",
    "    \"\"\"\n",
    "    Plot the segmentation results and metrics comparison.\n",
    "    \n",
    "    Parameters:\n",
    "    true_slice: Ground truth segmentation slice\n",
    "    pred_slice: Predicted segmentation slice (after Hungarian mapping)\n",
    "    metrics_dict: Dictionary containing metrics for each class\n",
    "    method_name: Name of the segmentation method\n",
    "    slice_idx: Index of the slice being visualized\n",
    "    \"\"\"\n",
    "    # Define colors and class names (note: we've remapped class 4 to class 3)\n",
    "    labels = [0, 1, 2, 3]\n",
    "    colors = ['black', 'green', 'yellow', 'lightblue']\n",
    "    class_names = ['Background (0)', 'NCR & NET (1)', 'Edema (2)', 'Enhancing Tumor (3)']\n",
    "    cmap_mask = ListedColormap(colors)\n",
    "    norm = BoundaryNorm([-0.5, 0.5, 1.5, 2.5, 3.5], cmap_mask.N)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Plot original segmentation\n",
    "    axs[0].imshow(pred_slice, cmap=cmap_mask, norm=norm)\n",
    "    axs[0].set_title(f'{method_name} Segmentation - Slice {slice_idx}')\n",
    "    axs[0].axis('off')\n",
    "    \n",
    "    # Plot ground truth\n",
    "    # Remap ground truth: 4 -> 3 for consistent visualization\n",
    "    true_remapped = true_slice.copy()\n",
    "    true_remapped[true_remapped == 4] = 3\n",
    "    axs[1].imshow(true_remapped, cmap=cmap_mask, norm=norm)\n",
    "    axs[1].set_title('Ground Truth (BraTS Segmentation)')\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    # Create metrics table\n",
    "    metrics_text = f\"{method_name} Metrics:\\n\\n\"\n",
    "    class_names = ['Background', 'Necrotic/Core', 'Edema', 'Enhancing']\n",
    "    for class_id, metrics in metrics_dict.items():\n",
    "        metrics_text += f\"{class_names[class_id]}:\\n\"\n",
    "        metrics_text += f\"  Dice = {metrics['Dice']:.4f}\\n\"\n",
    "        metrics_text += f\"  Sensitivity = {metrics['Sensitivity']:.4f}\\n\"\n",
    "        metrics_text += f\"  Precision = {metrics['Precision']:.4f}\\n\"\n",
    "        metrics_text += f\"  Hausdorff = {metrics['Hausdorff Distance']:.4f}\\n\\n\"\n",
    "    \n",
    "    # Display metrics\n",
    "    axs[2].text(0.1, 0.5, metrics_text, fontsize=10, verticalalignment='center')\n",
    "    axs[2].set_title('Evaluation Metrics')\n",
    "    axs[2].axis('off')\n",
    "    \n",
    "    # Add legend\n",
    "    patches = [mpatches.Patch(color=colors[i], label=class_names[i]) for i in range(len(labels))]\n",
    "    axs[1].legend(handles=patches, loc='lower left', bbox_to_anchor=(1.05, 0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def compute_unet_metrics(true_labels, pred_probabilities, threshold=0.5):\n",
    "    \n",
    "    Compute metrics for U-Net output.\n",
    "    pred_probabilities: Probability maps [H, W, 4] or [H, W, D, 4]\n",
    "    \n",
    "    # Remap ground truth: 4 -> 3 (if needed)\n",
    "    true_processed = true_labels.copy()\n",
    "    true_processed[true_processed == 4] = 3\n",
    "    \n",
    "    # Get predicted labels by taking argmax of probability maps\n",
    "    pred_labels = np.argmax(pred_probabilities, axis=-1)\n",
    "    \n",
    "    # Flatten arrays\n",
    "    true_flat = true_processed.flatten()\n",
    "    pred_flat = pred_labels.flatten()\n",
    "    \n",
    "    # Compute metrics without Hungarian mapping\n",
    "    metrics_dict = {}\n",
    "    for class_id in range(4):  # 4 classes after remapping\n",
    "        true_binary = (true_processed == class_id)\n",
    "        pred_binary = (pred_labels == class_id)\n",
    "        \n",
    "        # Dice Coefficient\n",
    "        intersection = np.logical_and(true_binary, pred_binary).sum()\n",
    "        dice = (2. * intersection) / (true_binary.sum() + pred_binary.sum() + 1e-6)\n",
    "        \n",
    "        # Sensitivity (Recall)\n",
    "        sensitivity = intersection / (true_binary.sum() + 1e-6)\n",
    "        \n",
    "        # Precision\n",
    "        precision = intersection / (pred_binary.sum() + 1e-6)\n",
    "        \n",
    "        # Hausdorff Distance\n",
    "        hd = np.nan\n",
    "        if np.any(true_binary) and np.any(pred_binary):\n",
    "            coords_true = np.argwhere(true_binary)\n",
    "            coords_pred = np.argwhere(pred_binary)\n",
    "            hd = max(directed_hausdorff(coords_true, coords_pred)[0],\n",
    "                     directed_hausdorff(coords_pred, coords_true)[0])\n",
    "        \n",
    "        metrics_dict[class_id] = {\n",
    "            'Dice': dice,\n",
    "            'Sensitivity': sensitivity,\n",
    "            'Precision': precision,\n",
    "            'Hausdorff Distance': hd\n",
    "        }\n",
    "    \n",
    "    return metrics_dict, pred_labels\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(metrics_dict, method_name):\n",
    "    print(f\"\\n{method_name} Metrics:\")\n",
    "    class_names = ['Background', 'Necrotic/Core', 'Edema', 'Enhancing']\n",
    "    for class_id, metrics in metrics_dict.items():\n",
    "        print(f\"{class_names[class_id]}:\")\n",
    "        print(f\"  Dice = {metrics['Dice']:.4f}\")\n",
    "        print(f\"  Sensitivity = {metrics['Sensitivity']:.4f}\")\n",
    "        print(f\"  Precision = {metrics['Precision']:.4f}\")\n",
    "        print(f\"  Hausdorff Distance = {metrics['Hausdorff Distance']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image segmentation is a kind of task in which Machine Learning has shown to be vastly superior to older algorithmical methods. The task at hand, due to the heterogeneous nature of the segmentation of MRI images, is yet another case in which modern deep learning techniques outperform other types of techniques. We still decided to try two non-ML segmentation methods, K-Means and Non-Negative Matrix Factorization (NMF), to obtain a baseline level performance for comparison with the U-Net architecture that will be applied later on in the project.\n",
    "\n",
    "Our application of these methods is barebones. While researchers trying to apply these methodologies would often use them in combination with a series of filters, edge detectors, thresholding methods that could increase the performances (as well as requiring a lot of manual labor and heuristics), here we limit ourselves to applying this techniques directly.\n",
    "\n",
    "In very simple segmentation tasks, using non-ML methods can still sometimes be useful, especially when there isn't enough data to train complex models on and/or when the segmentation is so simple that one can prefer techniques which are computationally much cheaper. In the context of medical imaging however, given the high precision needed to obtain actually usable software, classical techniques don't really find any application if not in pre- or post-processing of other algorithms input/output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the K-Means algorithm, we extract a certain slice (corresponding to a mask showing every kind of tumoral tissue) from a patient data for every modality. For example, the slice 95 from patient 355 for T1, T1ce, T2 and T2-FLAIR. We then unite these 4 slices into a single 240x240x4 array which is reshaped into a 57600x4 array. This is necessary because K-Means works on a 2D array representing the observations and corresponding features. We then visualize the clusters the algorithm found.\n",
    "\n",
    "The result strongly depends on the specific initialization. If the initial centroids all fall into the background section, than K-Means will cluster the whole image as a single cluster. If the initialization is more varied, we obtain more varied results. Because of the non-semantic nature of K-Means, some of the divisions it finds, like the one between CSF and healthy brain, while anatomically correct, are not of interest for the task at hand. Relative to the tumoral tissue, while it doesn't do a perfect job and while it can't always distinguish ET from ED, it is sometimes able to segment the different parts of the disease with a moderate level of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametri\n",
    "slice_idx = 95\n",
    "k = 4  # Numero di cluster per K-Means\n",
    " \n",
    "#Slice delle differenti modalità\n",
    "flair_slice = test_image_flair[:, :, slice_idx]\n",
    "t1_slice = test_image_t1[:, :, slice_idx]\n",
    "t1ce_slice = test_image_t1ce[:, :, slice_idx]\n",
    "t2_slice = test_image_t2[:, :, slice_idx]\n",
    "mask_slice = test_image_seg[:, :, slice_idx]\n",
    " \n",
    "# Le slice vengono congiunte attraverso il metodo stack, che unisce più array lungo una\n",
    "# nuova dimensione. Con un reshape, ogni pixel viene rappresentato da un vettore di 4 valori (uno per ogni modalità)\n",
    "\n",
    "X = np.stack([flair_slice, t1_slice, t1ce_slice, t2_slice], axis=-1) #240x240x4\n",
    "print(\"Shape of X:\", X.shape)\n",
    "X_reshaped = X.reshape((-1, 4)).astype(np.float32) #240x240=57600 -> 57600x4, KMeans lavora su un array 2D\n",
    "print(\"Shape of X_reshaped:\", X_reshaped.shape)\n",
    "\n",
    "# Applica K-Means\n",
    "km = KMeans(n_clus=k)\n",
    "km.fit(X_reshaped)\n",
    "clusters = km.getClusters()\n",
    " \n",
    "# Ritorniamo alla dimensione 240x240\n",
    "segmented_image = clusters.reshape(flair_slice.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As remarked above, K-Means is non-semantic. It is an unsupervised technique. This means that the clusters found need to be manually associated, in an heuristic manner, to ground truth labels. To make this association we take advantage of the hungarian algorithm. This then enables us to calculate the specificity, precision and Dice Coefficient. We also define a function allowing us to calculate the symmetric Hausdorff distance. These metrics work in tandem, in the sense that even having a good overlap between the ground truth label and the prediction doesn't guarantee that a section of the ground truth, small in area, may lie \"far\" from the predicted mask. The hausdorff distance, here measured in pixels, allows us to check the \"maximum minimum\" distance between the two masks, informing us in the case that a section of the mask in not being correctly classified by the model even if the total overlap is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the same slice from ground truth that was used for K-Means\n",
    "true_slice = test_image_seg[:, :, slice_idx]\n",
    "\n",
    "# Compute metrics for this specific slice\n",
    "kmeans_metrics, kmeans_mapped = compute_metrics(true_slice, segmented_image)\n",
    "\n",
    "# Print the results\n",
    "print_metrics(kmeans_metrics, \"K-Means\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_segmentation_comparison(true_slice, kmeans_mapped, kmeans_metrics, \"K-Means\", slice_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we repeat a process that is completely analogous to the previous K-Means section. We fit a \"classical\" model, namely Non-Negative Matrix Factorization, on an example slice to obtain a \"classical\" baseline perfomance level. The same metrics are used. We can see that NMF performs worse than K-Means, with much more confused clusters and consequently mostly worse metrics (with the exception of the background)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Parameters (use the same as K-Means)\n",
    "slice_idx = 95\n",
    "n_components = 4\n",
    "\n",
    "# Prepare the data (same as for K-Means)\n",
    "flair_slice = test_image_flair[:, :, slice_idx]\n",
    "t1_slice = test_image_t1[:, :, slice_idx]\n",
    "t1ce_slice = test_image_t1ce[:, :, slice_idx]\n",
    "t2_slice = test_image_t2[:, :, slice_idx]\n",
    "mask_slice = test_image_seg[:, :, slice_idx]\n",
    "\n",
    "X = np.stack([flair_slice, t1_slice, t1ce_slice, t2_slice], axis=-1)\n",
    "X_reshaped = X.reshape((-1, 4)).astype(np.float32)\n",
    "\n",
    "# Apply NMF\n",
    "model = NMF(n_components=n_components, init='nndsvda', random_state=42)\n",
    "W = model.fit_transform(X_reshaped)\n",
    "    \n",
    "# Assign each pixel to the component with the highest value\n",
    "nmf_segmented_flat = np.argmax(W, axis=1)\n",
    "nmf_segmented_image = nmf_segmented_flat.reshape(flair_slice.shape)\n",
    "\n",
    "# Compute metrics\n",
    "nmf_metrics, nmf_mapped = compute_metrics(mask_slice, nmf_segmented_image)\n",
    "\n",
    "# Print metrics\n",
    "print_metrics(nmf_metrics, \"NMF\")\n",
    "\n",
    "# Plot the results\n",
    "plot_segmentation_comparison(mask_slice, nmf_mapped, nmf_metrics, \"NMF\", slice_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we only really loaded data from patient 355. Now we need to prepare all the data at our disposal for the training and testing phases of the ML pipeline. The first thing we do is subdivide the 369 patients into training set (250 patients, 68% of the total), validation set (74 patients, 20%) and test set (45 patients, 12%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista degli ID (ultime parti dei path delle directory)\n",
    "train_and_test_ids = [f.name for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()]\n",
    "\n",
    "# 80% train+test, 20% validation\n",
    "train_test_ids, val_ids = train_test_split(train_and_test_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split train+test in 85% train (68% del totale), 15% test (12% del totale)\n",
    "train_ids, test_ids = train_test_split(train_test_ids, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"Train length: {len(train_ids)} pazienti\")\n",
    "print(f\"Validation length: {len(val_ids)} pazienti\")\n",
    "print(f\"Test length: {len(test_ids)} pazienti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(\n",
    "        [len(train_ids), len(val_ids), len(test_ids)],\n",
    "        labels=['Train', 'Validation', 'Test'],\n",
    "       )\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Data Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data as is needs to be preprocessed before being used in the training of the model. To do this, we define a DataGenerator from Keras to achieve all of the steps in an organized way.\n",
    "\n",
    "In the first cell, we define the number of slices to utilize and the starting index of these interval. We decided to use about 100 slices per modality. This comes down to the fact that many of the slices do not contain sections of the tumour and therefore do not contain information useful for the training of our segmentation model. This allows us , given that we train our model in trasverse direction, to reduce the computations per modalities by about a third (100/155 = 64,5%).\n",
    "\n",
    "Another parameter defined in the first cell is the image size. Instead of using the original images, whose dimension along the trasverse direction is 240x240, we resize them down to 128x128. This allows to obtain a resolution in powers of 2, which makes applying max-pooling layers straighforward. Also, this strongly reduces computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENT_CLASSES = {\n",
    "    0 : 'Background',\n",
    "    1 : 'Necrotic/Non-Enhancing', \n",
    "    2 : 'Edema',\n",
    "    3 : 'Enhancing'\n",
    "}\n",
    "\n",
    "# Select Slices and Image Size\n",
    "VOLUME_SLICES = 100\n",
    "VOLUME_START_AT = 22 \n",
    "IMG_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size # Numero di pazienti per batch\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels # Numero di modalità (FLAIR, T1ce)\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one batch of data'\n",
    "        # Generates indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        Batch_ids = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generates data\n",
    "        X, y = self.__data_generation(Batch_ids)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, Batch_ids):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n",
    "        y = np.zeros((self.batch_size*VOLUME_SLICES, 240, 240))\n",
    "        Y = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, 4))\n",
    "\n",
    "\n",
    "        # Generate data\n",
    "        for c, i in enumerate(Batch_ids):\n",
    "            case_path = os.path.join(TRAIN_DATASET_PATH, i) # i is the index of the patient\n",
    "\n",
    "            data_path = os.path.join(case_path, f'{i}_flair.nii');\n",
    "            flair = nib.load(data_path).get_fdata()\n",
    "\n",
    "            data_path = os.path.join(case_path, f'{i}_t1ce.nii');\n",
    "            t1ce = nib.load(data_path).get_fdata()\n",
    "\n",
    "            data_path = os.path.join(case_path, f'{i}_seg.nii');\n",
    "            seg = nib.load(data_path).get_fdata()\n",
    "\n",
    "            for j in range(VOLUME_SLICES): # Loading the resized T1ce, FLAIR and standard segmentation\n",
    "                 X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
    "                 X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(t1ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
    "\n",
    "                 y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n",
    "\n",
    "        y[y==4] = 3; # Label 4 to 3\n",
    "        mask = tf.one_hot(y, 4); # One-hot encoding to avoid any hierarchy\n",
    "        Y = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE)); #Resizing bilinearly (soft masks)\n",
    "        return X/np.max(X), Y #Normalize the full volume of the batch (Batchsize*VOLUME_SLICES)\n",
    "                              #Example: batch_size=2, VOLUME_SLICES=100 -> 200 slices normalized together\n",
    "                              \n",
    "training_generator = DataGenerator(train_ids)\n",
    "valid_generator = DataGenerator(val_ids)\n",
    "test_generator = DataGenerator(test_ids)\n",
    "\n",
    "\n",
    "print(\"Numero di batch nel training set:\", len(training_generator))\n",
    "print(\"Numero di batch nel validation set:\", len(valid_generator))\n",
    "print(\"Numero di batch nel test set:\", len(test_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to display one slice and its segmentation\n",
    "def display_slice_and_segmentation(flair, t1ce, segmentation):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
    "\n",
    "    axes[0].imshow(flair, cmap='gray')\n",
    "    axes[0].set_title('Flair')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(t1ce, cmap='gray')\n",
    "    axes[1].set_title('T1CE')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(segmentation) # Displaying segmentation\n",
    "    axes[2].set_title('Segmentation')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Retrieve a batch from the training generator\n",
    "X_batch, Y_batch = training_generator[9] #With batch_size=1, this is the 9th patient\n",
    "\n",
    "# Extract Flair, T1CE, and segmentation from the batch\n",
    "flair_batch = X_batch[:, :, :, 0]\n",
    "t1ce_batch = X_batch[:, :, :, 1]\n",
    "segmentation_batch = np.argmax(Y_batch, axis=-1)  # Convert one-hot encoded to categorical\n",
    "\n",
    "# Extract a slice from Flair, T1CE, and segmentation\n",
    "slice_index = 50  # Indexing starts from 0\n",
    "slice_flair = flair_batch[slice_index]\n",
    "slice_t1ce = t1ce_batch[slice_index]\n",
    "slice_segmentation = segmentation_batch[slice_index]\n",
    "\n",
    "# Display the 50th slice and its segmentation\n",
    "display_slice_and_segmentation(slice_flair, slice_t1ce, slice_segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(train_generator):\n",
    "    \"\"\"\n",
    "    Calculate class weights based on training data distribution\n",
    "    \"\"\"\n",
    "    # Count pixels per class\n",
    "    class_counts = np.zeros(4)  # background, necrotic, edema, enhancing\n",
    "    \n",
    "    for i in range(len(train_generator)):\n",
    "        _, y_batch = train_generator[i]\n",
    "        for class_idx in range(4):\n",
    "            class_counts[class_idx] += np.sum(y_batch[:, :, :, class_idx])\n",
    "    \n",
    "    # Calculate weights (inverse of frequency)\n",
    "    total_pixels = np.sum(class_counts)\n",
    "    class_weights = total_pixels / (4.0 * class_counts)\n",
    "    \n",
    "    # Normalize weights\n",
    "    class_weights = class_weights / np.sum(class_weights)\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "# Calculate class weights from your training data\n",
    "class_weights = calculate_class_weights(training_generator)\n",
    "print(f\"Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_categorical_crossentropy(class_weights):\n",
    "    \"\"\"\n",
    "    Custom loss function with class weighting\n",
    "    class_weights: list of weights for each class [background, necrotic, edema, enhancing]\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        # Scale predictions so that the class probabilities of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        \n",
    "        # Calculate loss with class weights\n",
    "        loss = y_true * K.log(y_pred) * class_weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean IoU for tumor classes only\n",
    "def mean_iou_tumor(y_true, y_pred):\n",
    "    y_true_tumor = y_true[:, :, :, 1:]\n",
    "    y_pred_tumor = y_pred[:, :, :, 1:]\n",
    "    \n",
    "    intersection = K.sum(K.round(K.clip(y_true_tumor * y_pred_tumor, 0, 1)))\n",
    "    union = K.sum(K.round(K.clip(y_true_tumor + y_pred_tumor, 0, 1))) - intersection\n",
    "    iou = intersection / (union + K.epsilon())\n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define per class evaluation of dice coef\n",
    "def dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):\n",
    "    intersection = K.sum(K.abs(y_true[:,:,:,1] * y_pred[:,:,:,1]))\n",
    "    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,1])) + K.sum(K.square(y_pred[:,:,:,1])) + epsilon)\n",
    "\n",
    "def dice_coef_edema(y_true, y_pred, epsilon=1e-6):\n",
    "    intersection = K.sum(K.abs(y_true[:,:,:,2] * y_pred[:,:,:,2]))\n",
    "    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,2])) + K.sum(K.square(y_pred[:,:,:,2])) + epsilon)\n",
    "\n",
    "def dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):\n",
    "    intersection = K.sum(K.abs(y_true[:,:,:,3] * y_pred[:,:,:,3]))\n",
    "    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,3])) + K.sum(K.square(y_pred[:,:,:,3])) + epsilon) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_dice_tumor(y_true, y_pred):\n",
    "    dice_nec = dice_coef_necrotic(y_true, y_pred)\n",
    "    dice_ed = dice_coef_edema(y_true, y_pred)\n",
    "    dice_enh = dice_coef_enhancing(y_true, y_pred)\n",
    "    return (dice_nec + dice_ed + dice_enh) / 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(inputs, ker_init, dropout):\n",
    "    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(inputs)\n",
    "    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv1)\n",
    "\n",
    "    pool = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool)\n",
    "    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n",
    "\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv2)\n",
    "\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv3)\n",
    "\n",
    "\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool4)\n",
    "    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv5)\n",
    "    drop5 = Dropout(dropout)(conv5)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv9)\n",
    "\n",
    "    up = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv9))\n",
    "    merge = concatenate([conv1,up], axis = 3)\n",
    "    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge)\n",
    "    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n",
    "\n",
    "    conv10 = Conv2D(4, (1,1), activation = 'softmax')(conv)\n",
    "\n",
    "    return Model(inputs = inputs, outputs = conv10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input((IMG_SIZE, IMG_SIZE, 2))\n",
    "model = build_unet(input_layer, 'he_normal', 0.2)\n",
    "\n",
    "# Compile with weighted loss and tumor-focused metrics\n",
    "model.compile(\n",
    "    loss=weighted_categorical_crossentropy(class_weights),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        mean_dice_tumor\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_mean_dice_tumor',  # Monitor tumor dice instead of loss\n",
    "        factor=0.2,\n",
    "        patience=2, \n",
    "        min_lr=0.000001, \n",
    "        verbose=1,\n",
    "        mode='max'  # We want to maximize dice\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='model_{epoch:02d}-{val_mean_dice_tumor:.6f}.weights.h5', \n",
    "        monitor='val_mean_dice_tumor',\n",
    "        verbose=1, \n",
    "        save_best_only=True, \n",
    "        save_weights_only=True,\n",
    "        mode='max'\n",
    "    ),\n",
    "    CSVLogger('training_tumor_focused.log', separator=',', append=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    training_generator,\n",
    "    epochs=35,\n",
    "    steps_per_epoch=len(train_ids),\n",
    "    callbacks=callbacks,\n",
    "    validation_data=valid_generator\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
